http {
  interface = "0.0.0.0"
  port = 6103
}

# we need to have this section in every module
# Plus, logback.xml in resource of every module
# Put into common module since not working properly
# Although logging system does see config files in both paths
akka {
    # set this on to debug logging system itself
    # useful when you not sure which config is loaded
    log-config-on-start = off
    loggers = ["akka.event.Logging$DefaultLogger"]
    # have to set this in order make sure logs are filtered using xml config before enter log bus
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    loglevel = "INFO"

    # By default messages sent to dead letters are logged at info level
    # We turned it off to avoid flooding the the logs when busy
    # Alternatively, we can set log-dead-letters = 10 i.e. only display 10 dead letters
    log-dead-letters = off
    # We don't want to see dead letters during system shutdown
    log-dead-letters-during-shutdown = off
}


indexer {
  readSnapshots = false
  alwaysReindex = false
  makeSnapshots = false
  connectionRetries = 10
  requestThrottleMs = 1000

  # Setting indexingBufferSize to be equal to indexingMaxBatchSize is recommended. This is because
  # the database crawling is usually faster than the elastic search bulk indexing . While the
  # current bulk indexing is underway, the crawling can quickly fill the stream buffer. When the
  # current bulk indexing is completed, there will be just enough datasets for the next round.
  indexingBufferSize = 500
  indexingMaxBatchSize = 500

  # It is recommended to set this value to a small value, such as 100 milliseconds, because this
  # delay is used in both indexing and re-indexing.
  # For the indexing, there are likely not many datasets to be bulk indexed; therefore it only needs
  # to wait for a short period time before starting the bulk indexing.
  # For the re-indexing, because the stream buffer is usually full (except for the beginning),
  # containing enough datasets for the next round of bulk indexing; therefore it is likely to get
  # the required bulk of datasets in a very short period of time.
  indexingInitialBatchDelayMs = 100
}

crawler {
  # In the current implementation, the initial crawling will fetch max of
  # streamControllerSourceBufferSize datasets. All the following crawlings will fetch max of
  # streamControllerSourceBufferSize/2 datasets. Set this size properly so that each crawling will
  # not fetch too many datasets. Otherwise an EntityStreamSizeException might occur.
  # The value of 100 is recommended.
  streamControllerSourceBufferSize = 100
}

registry {
  registerForWebhooks = true
  webhookId = "indexer"
  webhookUrl = "http://localhost:6103/v0/registry-hook"
  baseUrl = "http://localhost:6101"
  readOnlyBaseUrl = "http://localhost:6101"
}

regionLoading {
  cachePath = "~/regions"
  regionBufferMb = 50
}

elasticSearch {
  replicaCount = 0
  shardCount = 1

  connectTimeout = 30000
  # In apache HC project source code:
  # https://github.com/apache/httpcomponents-core/blob/fa857dccf17d0c7a402139bda740d45490ba81bd/httpcore-nio/src/main/java/org/apache/http/impl/nio/reactor/AbstractIOReactor.java#L492
  # It actually measures the session time. i.e. elasticsearch must complete the request within this time
  # This should be a bigger value for the bulk request
  socketTimeout = 600000
  maxRetryTimeout = 30000

  snapshotRepo {
    type = "fs"

    types {
      fs {
        location = "~/snapshots"
      }
    }
  }
}

auth {
    userId = "00000000-0000-4000-8000-000000000000"
}
