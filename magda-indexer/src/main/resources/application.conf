http {
  interface = "0.0.0.0"
  port = 6103
}

time {
    defaultOffset = "+10:00"
}

# we need to have this section in every module
# Plus, logback.xml in resource of every module
# Put into common module since not working properly
# Although logging system does see config files in both paths
akka {
    # set this on to debug logging system itself
    # useful when you not sure which config is loaded
    log-config-on-start = off
    loggers = ["akka.event.Logging$DefaultLogger"]
    # have to set this in order make sure logs are filtered using xml config before enter log bus
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    loglevel = "INFO"

    # By default messages sent to dead letters are logged at info level
    # We turned it off to avoid flooding the the logs when busy
    # Alternatively, we can set log-dead-letters = 10 i.e. only display 10 dead letters
    log-dead-letters = off
    # We don't want to see dead letters during system shutdown
    log-dead-letters-during-shutdown = off

    http {
        server {
            request-timeout = 90s
            idle-timeout = 120s
        }

        client {
            # User might increase this setting when encounter connection reset error for downloading large region files
            idle-timeout = 180s
        }

    }
}


indexer {
  # https://www.elastic.co/guide/en/elasticsearch/reference/6.8/index-modules.html#dynamic-index-settings
  # Default to 1s
  # we set this settings for test cases only
  # refreshInterval = -1
  readSnapshots = false
  alwaysReindex = false
  makeSnapshots = false
  connectionRetries = 10
  requestThrottleMs = 1000

  # Setting indexingBufferSize to be equal to indexingMaxBatchSize is recommended. This is because
  # the database crawling is usually faster than the elastic search bulk indexing . While the
  # current bulk indexing is underway, the crawling can quickly fill the stream buffer. When the
  # current bulk indexing is completed, there will be just enough datasets for the next round.
  indexingBufferSize = 500
  indexingMaxBatchSize = 500

  # It is recommended to set this value to a small value, such as 100 milliseconds, because this
  # delay is used in both indexing and re-indexing.
  # For the indexing, there are likely not many datasets to be bulk indexed; therefore it only needs
  # to wait for a short period time before starting the bulk indexing.
  # For the re-indexing, because the stream buffer is usually full (except for the beginning),
  # containing enough datasets for the next round of bulk indexing; therefore it is likely to get
  # the required bulk of datasets in a very short period of time.
  indexingInitialBatchDelayMs = 100

  # when set true, indexer might auto recrawl all datasets on starting up depends on whether index is empty or webhook status.
  # Set this option to false will disable any possible auto crawl action on starting up.
  allowAutoCrawlOnStartingUp = true

  # When hybrid search is on, it takes one HTTP request to generate embedding vectors for the dataset.
  # Thus, we will want to set to a number that matches the embedding service's capability.
  # When hybrid search is set to off, we should set this value to much higher number as there would be no HTTP requests made (only local data conversion).
  datasetConversionParallelism = 5

  # Whether to enable async mode for handling registry webhooks.
  asyncWebhook = true

  stream-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 4
    }
    throughput = 1
  }

  main-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 8
    }
    throughput = 1
  }

}

blocking-io-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    fixed-pool-size = 4
  }
  throughput = 1
}

crawler {
  # In the current implementation, the initial crawling will fetch max of
  # streamControllerSourceBufferSize datasets. All the following crawlings will fetch max of
  # streamControllerSourceBufferSize/2 datasets. Set this size properly so that each crawling will
  # not fetch too many datasets. Otherwise an EntityStreamSizeException might occur.
  # The value of 100 is recommended.
  streamControllerSourceBufferSize = 100

  main-dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 4
    }
    throughput = 1
  }
}

registry {
  registerForWebhooks = true
  webhookId = "indexer"
  webhookUrl = "http://localhost:6103/v0/registry-hook"
  baseUrl = "http://localhost:6101"
  readOnlyBaseUrl = "http://localhost:6101"
}

regionLoading {
  cachePath = "~/regions"
  regionBufferMb = 50
}

elasticSearch {
  # Dynamic index setting: `number_of_replicas`. Can be updated on a live index.
  # This config only applies to new indices when indexer creates them. Indexer won't attempt to apply any changes to existing indices.
  # You should update the setting on existing indices using the API: PUT /[index_name]/_settings
  # How many shards and replicas to use when creating a new index.
  # Replica shards can help with "Data Redundancy and Availability", "Search Performance" and "Resilience to Node Failure".
  # It won't help with "Write Performance" (a higher `number_of_replicas` can slow down indexing performance).
  # More replicas also mean more storage space required.
  # Generally, For a single-node cluster, `number_of_replicas` should be set to 0, as there are no other nodes to hold replicas.
  # For a multi-node cluster, `number_of_replicas` should be set to at least 1 to ensure data redundancy and high availability.
  # If search performance is a priority, consider increasing the `number_of_replicas`. However, keep in mind the trade-off with write performance and storage requirements.
  # Always ensure that your cluster has enough capacity to store the additional replicas.
  # `number_of_replicas` should not be higher than no. of data nodes in the cluster. Otherwise, the indices will be in yellow state (all not all replicas could be allocated).
  replicaCount = 0

  # Static index setting `number_of_shards`. The number of primary shards that an index should have.
  # Defaults to 1. This setting can only be set at index creation time. It cannot be changed on a closed index.
  shardCount = 1

  connectTimeout = 30000
  # In apache HC project source code:
  # https://github.com/apache/httpcomponents-core/blob/fa857dccf17d0c7a402139bda740d45490ba81bd/httpcore-nio/src/main/java/org/apache/http/impl/nio/reactor/AbstractIOReactor.java#L492
  # It actually measures the session time. i.e. elasticsearch must complete the request within this time
  # This should be a bigger value for the bulk request
  socketTimeout = 600000
  maxRetryTimeout = 30000

  snapshotRepo {
    type = "fs"

    types {
      fs {
        location = "~/snapshots"
      }
    }
  }
}

auth {
    userId = "00000000-0000-4000-8000-000000000000"
}

authApi {
    baseUrl = "http://localhost:6104"
}
